# credit-default
### Examining default of credit card clients dataset from UCI ML Repo

This is a simple simulation where I analyze the default of credit card client dataset from UCI ML Repo. I start by performing an exploratory data analysis. By using pandas functions I can see the total number of clients (rows), the mean value of each column, and standard deviation (variance) to see how much the values vary. Three of these columns stuck out as categorical variables: sex, marriage, and education. There were a few unassigned codes to these variables, so I reassigned these to the code for 'other'. I generated some density plots to further show the distribution of data and how it is related to defaulting on a loan. It shows that lower credit limits have a higher rate of default than higher lines of credit. The most commonly issued line of credit is $50,000.

I decided to go with a Decision Tree Classifier for this simulation as it is simple to use and can handle numerical and categorical variables. After fitting and training the model, the initial f1 score metric for running the model out of the box wasÂ 0.48765803732691154. However, I felt I could easily improve on this by using GridSearchCV for hyperparameter tuning optimization. GridSearchCV helped determine the optimal parameters in this case were max_depth of 3, max_leaf_nodes of 20, min_sample_split of 2, etc. The f1 score bumped up to 0.541282233262361.

In the future and given more time, I would like to improve on the model's performance metric.I would like to spend more time feature engineering. Perhaps the sex and marriage column could be combined into a separate feature, ie. married men or unmarried women, or looking at how specific age groups affect the default rate. I would perform a principal component analysis to look at correlations between limit balance, bill_amt, and pay_amt. I could also apply some sampling techniques and separate the dataset into majority (non-default) and minority (default) classes. Comparing the basic decision tree's performance to other bagging and boosting ensemble classifiers such as Random Forest and XGBoost would also be of interest to me as they are generally less likely to overfit and are more resilient to small changes in the data.
